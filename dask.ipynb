{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3fb50e-51bb-4efb-8124-a5a02bb4615f",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "[Dask](https://dask.org/) is a library for parallel processing in Python, with a specific focus on analytic and scientific computing. Compared to Spark, it is more familiar to Python-oriented data scientists. In this notebook, we'll spin up an ad-hoc Dask cluster on top of CML sessions using the CML [Workers API](https://docs.cloudera.com/machine-learning/cloud/distributed-computing/topics/ml-workers-api.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b817ff-3683-40ac-addd-870d31e592a2",
   "metadata": {},
   "source": [
    "## Set up a Dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7c6ee-86ba-4641-bb65-427033fde18a",
   "metadata": {},
   "source": [
    "First, we install and import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140143f6-8bbc-49eb-8e3b-c89902eeb5e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install dask[complete]==2021.2.0 dask-ml==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f1964-7860-4eaa-80a2-12802d555a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cdsw\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask_ml as dm\n",
    "import dask_ml.datasets\n",
    "import dask_ml.linear_model\n",
    "\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ce70a-2c6b-40c5-9713-7faab233cde0",
   "metadata": {},
   "source": [
    "### Start Dask scheduler\n",
    "We need to make two directories required by Dask. Dask uses these directories to share network information between the scheduler and workers. From our, user, perspective, we can create them and forget them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e827240-2737-4434-b691-7347969057b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"_scheduler_\", exist_ok=True)\n",
    "os.makedirs(\"_worker_\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693b0d2-924c-4d3f-87b5-be1a72ed78c0",
   "metadata": {},
   "source": [
    "We start a Dask scheduler as a CDSW worker process. We do this with cdsw.launch_workers, which spins up another session on our cluster and runs the command we provide â€” in this case the Dask scheduler. The scheduler is responsible for coordinating work between the Dask workers we will attach. Later we'll start a Dask client in this notebook. The client talks to the scheduler, and the scheduler talks to the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462ec45-4b1a-4119-b2e3-c73bb3a0781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_scheduler = cdsw.launch_workers(\n",
    "    n=1,\n",
    "    cpu=1,\n",
    "    memory=2,\n",
    "    code=f\"!dask-scheduler --host 0.0.0.0 --dashboard-address 127.0.0.1:8090 --scheduler-file /home/cdsw/_scheduler_/dask.log\",\n",
    ")\n",
    "\n",
    "# Wait for the scheduler to start.\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db17f95-e99f-437c-8b59-e0505f5a56a1",
   "metadata": {},
   "source": [
    "We need the IP address of the CML worker with the scheduler on it, so we can connect the Dask workers to it. The IP is not returned in the dask_scheduler object (it's unknown at the launch of the scheduler), so we scan through the worker list and find the IP of the worker with the scheduler id. This returns a list, but there should be only one entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a2e4e-7046-4aa8-9e00-98ce1d1b48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_workers = cdsw.list_workers()\n",
    "scheduler_id = dask_scheduler[0][\"id\"]\n",
    "scheduler_ip = [\n",
    "    worker[\"ip_address\"] for worker in scheduler_workers if worker[\"id\"] == scheduler_id\n",
    "][0]\n",
    "\n",
    "scheduler_url = f\"tcp://{scheduler_ip}:8786\"\n",
    "\n",
    "scheduler_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dcb5c1-24a2-468a-92aa-95b201b5b78a",
   "metadata": {},
   "source": [
    "### Start Dask workers\n",
    "We're ready to grow our cluster. We start some more CML workers, each with one Dask worker process on it. We pass the scheduler URL we just found so that the scheduler can talk, and distribute work, to the workers.\n",
    "\n",
    "N_WORKERS determines the number of CML workers started (and thus the number of Dask workers running in those sessions). Increasing the number will start more workers. This will speed up the wall-clock time of the TPOT training process, by training more pipelines in parallel, but it uses more cluster resources. Exercise good judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220b142-79cc-452b-af43-85868096e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORKERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5bcdc8-3a71-4e42-874d-4161367a423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_workers = cdsw.launch_workers(\n",
    "    n=N_WORKERS,\n",
    "    cpu=1,\n",
    "    memory=2,\n",
    "    code=f\"!dask-worker {scheduler_url} --local-directory /home/cdsw/_worker_\",\n",
    ")\n",
    "\n",
    "# Wait for the workers to start.\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1d723-6d79-4b48-bb9f-89b5919d33f1",
   "metadata": {},
   "source": [
    "### Connect Dask client\n",
    "We have a Dask cluster running and distributed over CML sessions. Now we can start a local Dask client and connect it to our scheduler. This is the connection that lets us issue instructions to the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bd6bd-e2ad-4f28-b3a9-79a4a244d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(scheduler_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8148b5-6380-4967-b9d1-9545fa8bbd9f",
   "metadata": {},
   "source": [
    "We can view some stats about the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652817e-2e9f-48e9-8b59-bf654f3b3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8a63e-830a-4d2f-8ef3-674929cf45ba",
   "metadata": {},
   "source": [
    "The Dask scheduler hosts a dashboard so we can monitor the work it's doing. Here we construct the URL of dashboard, which is hosted on the scheduler worker. Clicking it should open the dashboard in a new browser window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d10d11-aef6-496e-816b-66a58817b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"//\".join(dask_scheduler[0][\"app_url\"].split(\"//\")) + \"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081918d-a7af-43ec-b5e5-8e49016f60b5",
   "metadata": {},
   "source": [
    "That's our Dask cluster set up, let's do something with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba279c3-06a3-4cfe-826b-0c8c7e3a3338",
   "metadata": {},
   "source": [
    "## Do some data science!\n",
    "\n",
    "Dask provides distributed equivalents to several popular and useful libraries in the Python data science ecosystem. Here we'll give a very brief demo of the Dask equivalents of [NumPy](https://numpy.org/) (Dask Array), [Pandas](https://pandas.pydata.org/) (Dask DataFrames), and [scikit-learn](https://scikit-learn.org/stable/) (Dask ML)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2dffcb-11ca-4f91-b3ed-6328bb85dd0b",
   "metadata": {},
   "source": [
    "### Dask Arrays\n",
    "\n",
    "We can instantiate a random multidimensional array like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d02a1-4617-4dd4-a513-0e9c84b8ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = da.random.random((10_000, 10, 10_000), chunks=1000)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03396a6d-5bf6-435f-9912-748f6b757404",
   "metadata": {},
   "source": [
    "Notice that this is lazily evaluated: the array would be around 7.5 GiB in memory, but we haven't computed anything yet. The `chunks` parameter controls data layout; above we're splitting it into 1000 chunks. Each chunk is a NumPy array. We can now queue up NumPy-like manipulation on it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9810fa-8a3f-4574-aa62-1698efe6995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these manipulations do not carry any special meaning\n",
    "array = (\n",
    "    da.reshape(array, (10_000, 100_000)) # reshape the array\n",
    "    .T                                   # transpose it\n",
    "    [:10, :1000]                         # take only the first 10 elements of the outer axis\n",
    ")\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6cd56-2a88-48b3-9ad6-900f497c49a5",
   "metadata": {},
   "source": [
    "Dask even includes parallel versions of much of the NumPy linalg functionality, so we can do, for instance, a singular value decomposition of our transformed array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b05ba97-41b0-4cd6-9753-b18b14a290ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = da.linalg.svd(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95749a2f-ed37-49dd-9d79-e950bf4e2730",
   "metadata": {},
   "source": [
    "The arrays we just computed with are distributed and lazily evaluated. To access their contents as a NumPy array, we must call `.compute()` explicitly. Be careful not to accidentally bring back an array that is bigger than the session memory, since that will crash the session. This computation will take a little time, and we can see the work happening over in the Dask Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925a53b-b831-454f-82bb-060eca85a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ea097-f8b1-44b0-b4c1-a41014f33f4d",
   "metadata": {},
   "source": [
    "### Dask DataFrames\n",
    "\n",
    "Dask DataFrames are extremely similar to Pandas DataFrames. In fact, Dask is really just co-ordinating Pandas objects under the hood. As such, we have access to most of the Pandas API, with the caveat that operations will be faster or slower depending on their degree of parallelizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596d421-b14b-420a-a477-e1af099fcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask provides a handy dataset for demo-ing itself\n",
    "df = dask.datasets.timeseries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc5b62-533b-47a9-912c-0ac66d2caa7d",
   "metadata": {},
   "source": [
    "We can take a peak at the head of the DataFrame, which will return the head of the first Pandas DataFrame in the Dask structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e99028-fed9-4869-9388-a23322702b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d173db3-7f38-4aaa-98ff-5748dc9838b5",
   "metadata": {},
   "source": [
    "We can do standard DataFrame operations, like finding the unique values of a column. This is an operation on distributed data, so we must call `.compute()` to collect the result. When we call `.head()`, the result is collected for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b8d60-284b-4975-a3d9-491afe2361a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df[\"name\"].unique().values\n",
    "names.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cbceb-4cb8-4222-9706-426418c1fd31",
   "metadata": {},
   "source": [
    "We can chain operations as usual. Once we've called `.compute()`, we're left with a Pandas DataFrame, and can call regular Pandas methods (like `.plot()`) on it.\n",
    "\n",
    "(There's no special meaning to the operations below. We're just taking the column-wise cumulative sum of some random numbers for a filtered set of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52000084-2b53-442a-979e-f09860652939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.name == \"Oliver\")][[\"x\", \"y\"]].cumsum().compute().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189184f-c048-43fb-b5d4-020b33816aa2",
   "metadata": {},
   "source": [
    "### Dask ML\n",
    "\n",
    "Dask ML supports several machine learning frameworks, mostly through scikit-learn integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6a7b2-13b2-4823-a2b0-936dd9f41d2a",
   "metadata": {},
   "source": [
    "First, generate a fake classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddcca18-440c-4cc1-a048-801ca1bcb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dm.datasets.make_classification(n_samples=100_000, chunks=1000, random_state=123)\n",
    "X = X.persist()\n",
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a567d-ed1c-4acf-a374-819632b271bc",
   "metadata": {},
   "source": [
    "And define a logistic regression model with L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a915c2-4205-4baf-a7e4-83893ca9fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = dm.linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b89356-7536-4e77-8e78-282451b87844",
   "metadata": {},
   "source": [
    "We can fit that on the distributed Dask dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1adec-8173-4d5e-a474-c7092a0f6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e3a78-1784-40b4-95c2-dfb30c9b3cac",
   "metadata": {},
   "source": [
    "And report our training loss. The trained algorithm is still a Dask object, so we must call `.compute()` to retrieve the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18830a23-bf5f-47f7-922c-95468220c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X, y).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d900b-4df6-4b5c-979f-038e77f6fc3f",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4e09f-d589-4483-a23a-3d6bc5be4af0",
   "metadata": {},
   "source": [
    "Now that we're done computing with our distributed Dask cluster, we should shut down those workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce896d3-2a7f-4f71-9029-4929ef20340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdsw.stop_workers(*[worker[\"id\"] for worker in dask_workers + dask_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6895dee-26f2-40b4-83f5-b95db1256ef3",
   "metadata": {},
   "source": [
    "***If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required notices. A copy of the Apache License Version 2.0 can be found [here](https://opensource.org/licenses/Apache-2.0).***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
